{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplog_mlm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRxcHVtUrCJ0",
        "outputId": "8021f0a9-8249-4d25-f82e-2ec70bcce702"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HYxTvtRoK4s",
        "outputId": "241bd60e-43b4-40dc-b147-f1089661f50f"
      },
      "source": [
        "%cd /content/drive/'My Drive'/'CS 247'/'Project'\n",
        "%ls"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CS 247/Project\n",
            "\u001b[0m\u001b[01;34mloglizer\u001b[0m/  \u001b[01;34mnulog\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-tFfwef_YYI"
      },
      "source": [
        "run_models = ['PCA', 'InvariantsMiner', 'LogClustering', 'IsolationForest', 'LR', \n",
        "              'SVM', 'DecisionTree']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO2f63HNqWaO",
        "outputId": "00f69925-9cde-4d36-f772-abaf2273749b"
      },
      "source": [
        "#!git clone https://github.com/logpai/loglizer.git\n",
        "#!git clone https://github.com/nulog/nulog.git\n",
        "%cd loglizer\n",
        "\n",
        "#!pip install -r requirements.txt"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CS 247/Project/loglizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_hB2qELrZeK"
      },
      "source": [
        "import sys\n",
        "sys.path.append('../')\n",
        "from loglizer import dataloader\n",
        "from loglizer.models import DeepLog\n",
        "from loglizer import preprocessing\n",
        "from loglizer.preprocessing import Vectorizer, Iterator\n",
        "sys.path.append('../nulog/logparser/')\n",
        "from NuLog import NuLogParser"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4cM1SwBr7JL"
      },
      "source": [
        "batch_size = 32\n",
        "hidden_size = 32\n",
        "num_directions = 2\n",
        "topk = 5\n",
        "train_ratio = 0.2\n",
        "window_size = 10\n",
        "epoches = 2\n",
        "num_workers = 2\n",
        "device = 2 "
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jHzIVlmr9Sl"
      },
      "source": [
        "#struct_log = '../MyData/' # The structured log file\n",
        "#label_file = '../MyData/anomaly_label.csv' # The anomaly label file\n",
        "curr_path = \"/content/drive/My Drive/CS 247/Project/loglizer\"\n",
        "#struct_log = '/content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS.npz'\n",
        "struct_log = '/content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS_100k.log_structured.csv'\n",
        "label_file = '/content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/anomaly_label.csv' # The anomaly label file"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zve346mRDJOz",
        "outputId": "e49d722a-18b7-4467-c8df-47f1849134ed"
      },
      "source": [
        "DATA = []\n",
        "DATA.append(dataloader.load_HDFS(struct_log,\n",
        "                                                           window='session', \n",
        "                                                           train_ratio=0.8,\n",
        "                                                           split_type='uniform'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== Input data summary ======\n",
            "Loading /content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS_100k.log_structured.csv\n",
            "Warning: Only split_type=sequential is supported                 if label_file=None.\n",
            "Total: 7940 instances, train: 6352 instances, test: 1588 instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-IZSL73ov0B",
        "outputId": "95789a6a-f62b-4144-fda6-97b23f428e2f"
      },
      "source": [
        "\n",
        "# log_tokenizer = NuLogParser.LogTokenizer()\n",
        "# masked_dataset = NuLogParser.MaskedDataset(DATA)\n",
        "print(DATA)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[((array([list(['E5', 'E5', 'E22', 'E5', 'E9', 'E11', 'E9', 'E11', 'E26', 'E26', 'E11', 'E9', 'E26']),\n",
            "       list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E26', 'E11', 'E9']),\n",
            "       list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26']),\n",
            "       ...,\n",
            "       list(['E5', 'E5', 'E5', 'E22', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26', 'E26', 'E26']),\n",
            "       list(['E5', 'E22', 'E5', 'E5', 'E11', 'E9', 'E11', 'E9', 'E9', 'E26', 'E26', 'E11', 'E26', 'E2']),\n",
            "       list(['E5', 'E22', 'E5', 'E5', 'E26', 'E11', 'E9', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9'])],\n",
            "      dtype=object), None), (array([list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E26', 'E11', 'E9', 'E11', 'E9']),\n",
            "       list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26']),\n",
            "       list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26']),\n",
            "       ...,\n",
            "       list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26']),\n",
            "       list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26']),\n",
            "       list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E26', 'E11', 'E9'])],\n",
            "      dtype=object), None),                        BlockId                                      EventSequence\n",
            "0     blk_-1608999687919862906  [E5, E22, E5, E5, E11, E11, E9, E9, E11, E9, E...\n",
            "1      blk_7503483334202473044  [E5, E5, E22, E5, E11, E9, E11, E9, E11, E9, E...\n",
            "2     blk_-3544583377289625738  [E5, E22, E5, E5, E11, E9, E11, E9, E11, E9, E...\n",
            "3     blk_-9073992586687739851  [E5, E22, E5, E5, E11, E9, E11, E9, E11, E9, E...\n",
            "4      blk_7854771516489510256  [E5, E5, E22, E5, E11, E9, E11, E9, E11, E9, E...\n",
            "...                        ...                                                ...\n",
            "7935  blk_-1445970677921829671  [E22, E5, E5, E5, E11, E9, E11, E9, E26, E26, ...\n",
            "7936  blk_-5943236831140622436  [E22, E5, E5, E5, E26, E26, E26, E11, E9, E11,...\n",
            "7937  blk_-5039164935117450945  [E22, E5, E5, E5, E26, E26, E11, E9, E11, E9, ...\n",
            "7938   blk_7379833155074044619  [E22, E5, E5, E5, E26, E26, E11, E9, E11, E9, ...\n",
            "7939   blk_8909107483987085802  [E22, E5, E5, E5, E26, E26, E11, E9, E11, E9, ...\n",
            "\n",
            "[7940 rows x 2 columns])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAjDpm8OUhiz"
      },
      "source": [
        "### N-gram baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7vswmzQUvSN"
      },
      "source": [
        "class N_gram(object):\n",
        "\n",
        "  def __init__(self, x_train, x_test, n = 3):\n",
        "    self.vocab = {}\n",
        "    self.ngram_counts = {}\n",
        "    #ids = set(x_train.SessionId)\n",
        "    #for id in ids:\n",
        "    for i,seq in enumerate(x_train.EventSequence):\n",
        "      m = len(seq)\n",
        "      for j in range(m - n + 1):\n",
        "        curr_ngram = seq[j:(j+n)]\n",
        "        curr_ngram_str = ''.join(curr_ngram)\n",
        "        if curr_ngram_str not in self.vocab:\n",
        "          self.vocab[curr_ngram_str] = 1\n",
        "        else:\n",
        "          self.vocab[curr_ngram_str] += 1\n",
        "        if y_train[i] == 0:\n",
        "          count = np.array([1, 0])\n",
        "        else:\n",
        "          count = np.array([0, 1])\n",
        "        prior_ngram_str = ''.join(curr_ngram[:-1])\n",
        "        if prior_ngram_str not in self.ngram_counts:\n",
        "          self.ngram_counts[prior_ngram_str] = count\n",
        "        else:\n",
        "          self.ngram_counts[prior_ngram_str] += count\n",
        "  \n",
        "  def eval(self, x_test, y_test):\n",
        "    preds = []\n",
        "    for i, seq in enumerate(x_test.EventSequence):\n",
        "      m = len(seq)\n",
        "      seq_log_prob = np.zeros(2)\n",
        "      for j in range(m - n + 1):\n",
        "        curr_ngram = seq[j:(j+n)]\n",
        "        curr_ngram_str = ''.join(curr_ngram)\n",
        "        denom = self.vocab.get(curr_ngram_str, 0)\n",
        "        if denom == 0:\n",
        "          seq_log_prob += np.log(0.5)\n",
        "        else:\n",
        "          prior_ngram_str = ''.join(curr_ngram[:-1])\n",
        "          num = self.ngram_counts.get(prior_ngram_str, np.zeros(2))\n",
        "          #print(i, prior_ngram_str, num)\n",
        "          seq_log_prob += np.log(np.array(num) / denom)\n",
        "      preds.append(np.argmax(seq_log_prob))\n",
        "    \n",
        "    preds = np.array(preds)\n",
        "    for sess in x_test.SessionId:\n",
        "      inds = x_test[x_test.SessionId == 11].index.tolist()\n",
        "      preds[inds[0]:inds[1]] = np.round(np.mean(preds[inds[0]:inds[1]])).astype(int)\n",
        "    \n",
        "    print(\"acc = \", np.mean(preds == y_test))\n",
        "    return preds\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSLzT1uNGdR0",
        "outputId": "e77d0705-118c-4655-941b-ad7d4df1d45a"
      },
      "source": [
        "trigram = N_gram(x_train, x_test, 3)\n",
        "trigram.eval(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc =  0.967373193319642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKV680cQ_P9h",
        "outputId": "eca8334d-0ed9-458b-e117-a633d6cc46c3"
      },
      "source": [
        "all_pred = np.zeros(y_test.shape)\n",
        "for i in range(1, 9):\n",
        "  ngram = N_gram(x_train, x_test, i)\n",
        "  pred = ngram.eval(x_test, y_test)\n",
        "  all_pred += pred\n",
        "\n",
        "all_pred /= 8\n",
        "print(\"acc = \", np.mean(all_pred == y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc =  0.967373193319642\n",
            "acc =  0.967373193319642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "acc =  0.21951408532568684\n",
            "acc =  0.967373193319642\n",
            "acc =  0.967373193319642\n",
            "acc =  0.967373193319642\n",
            "acc =  0.967373193319642\n",
            "acc =  0.967373193319642\n",
            "acc =  0.19777579726430813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgDkVKSyVflM"
      },
      "source": [
        "### Loglizer base models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIwvnJZHVxuC"
      },
      "source": [
        "from loglizer.models import *\n",
        "from loglizer import dataloader, preprocessing\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoYwA7hcbHAZ"
      },
      "source": [
        "structLog = pd.read_csv(struct_log, engine='c',\n",
        "                na_filter=False, memory_map=True)\n",
        "data_dict = OrderedDict()\n",
        "for idx, row in structLog.iterrows():\n",
        "  blkId_list = re.findall(r'(blk_-?\\d+)', row['Content'])\n",
        "  blkId_set = set(blkId_list)\n",
        "  for blk_Id in blkId_set:\n",
        "    if not blk_Id in data_dict:\n",
        "      data_dict[blk_Id] = []\n",
        "    data_dict[blk_Id].append(row['EventTemplate'])\n",
        "data_df = pd.DataFrame(list(data_dict.items()), columns=['BlockId', 'EventTemplate'])\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nn0u4HecSBb",
        "outputId": "418b3a54-9694-477a-e338-e0ef51c4b305"
      },
      "source": [
        "filter = '(\\s+blk_)|(:)|(\\s)'\n",
        "print (data_df['EventTemplate'].to_numpy()[0][0])\n",
        "data12 = data_df['EventTemplate'].to_numpy()[0]\n",
        "log_tokenizer = NuLogParser.LogTokenizer(filter)\n",
        "data_tokenized = []\n",
        "for i in range(0, len(data_df)):\n",
        "  for j in range (0, len(data_df.iloc[i].EventTemplate)):\n",
        "    tokenized = log_tokenizer.tokenize('<CLS> ' + data_df.iloc[i].EventTemplate[j])\n",
        "    data_tokenized.append(tokenized)\n",
        "# masked_dataset = NuLogParser.MaskedDataset(data12, log_tokenizer)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Receiving block <*> src: /<*> dest: /<*>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF1uZHf-wTGr"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler\n",
        "from torchvision import transforms, utils\n",
        "log_format =  '<Date> <Time> <Pid> <Level> <Component>: <Content>'\n",
        "k = 15\n",
        "\n",
        "log_parser = NuLogParser.LogParser(None, None, filter, k, log_format)\n",
        "# masked_dataset = NuLogParser.MaskedDataset(data_tokenized, log_tokenizer)\n",
        "masked_dataset.mask_percentage = 0.2\n",
        "\n",
        "transform_to_tensor = transforms.Lambda(lambda lst: torch.tensor(lst))\n",
        "\n",
        "train_data = NuLogParser.MaskedDataset(data_tokenized, log_tokenizer, transforms=transform_to_tensor)\n",
        "weights = train_data.get_sample_weights()\n",
        "# if self.num_samples != 0:\n",
        "#   train_sampler = WeightedRandomSampler(weights=list(weights), num_samples=self.num_samples, replacement=True)\n",
        "# if self.num_samples == 0:\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=5)\n",
        "\n",
        "test_data = NuLogParser.MaskedDataset(data_tokenized, log_tokenizer, mask_percentage=0.2, transforms=transform_to_tensor)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=5)\n",
        "# train_dataloader, test_dataloader = log_parser.get_dataloaders(data_tokenized)\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8D3VnMs0RV5",
        "outputId": "e87d743b-801b-4cfb-b8bb-90196f4e2ff3"
      },
      "source": [
        ""
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f0ced0ace10>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgxibRK_pVLs",
        "outputId": "a5b3e32d-1061-4f37-955a-8bc10a27180d"
      },
      "source": [
        "(x_tr, y_train), (x_te, y_test), data_df = dataloader.load_HDFS(struct_log,\n",
        "                                                           window='session', \n",
        "                                                           train_ratio=0.5,\n",
        "                                                           split_type='sequential')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== Input data summary ======\n",
            "Loading /content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS_100k.log_structured.csv\n",
            "Total: 7940 instances, train: 3970 instances, test: 3970 instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NDdw0vMpXiq",
        "outputId": "5df38736-82ca-4a0b-cabc-132380f91a0d"
      },
      "source": [
        "print (x_tr)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9'])\n",
            " list(['E5', 'E22', 'E5', 'E5', 'E11', 'E9', 'E9', 'E11', 'E9', 'E11', 'E26', 'E26', 'E26'])\n",
            " list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9'])\n",
            " ...\n",
            " list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26'])\n",
            " list(['E5', 'E22', 'E5', 'E5', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26', 'E26', 'E26'])\n",
            " list(['E22', 'E5', 'E5', 'E5', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26', 'E26', 'E26'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JESGo8ZLgjU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "outputId": "5edf8c4b-f81b-4ae6-9ebf-b39461920db9"
      },
      "source": [
        "(x_tr, y_train), (x_te, y_test), data_df = dataloader.load_HDFS(struct_log,\n",
        "                                                           window='session', \n",
        "                                                           train_ratio=0.5,\n",
        "                                                           split_type='uniform')\n",
        "benchmark_results = []\n",
        "for _model in run_models:\n",
        "  print('Evaluating {} on HDFS:'.format(_model))\n",
        "  if _model == 'PCA':\n",
        "      feature_extractor = preprocessing.FeatureExtractor()\n",
        "      x_train = feature_extractor.fit_transform(x_tr, term_weighting='tf-idf', \n",
        "                                                normalization='zero-mean')\n",
        "      model = PCA()\n",
        "      model.fit(x_train)\n",
        "\n",
        "  elif _model == 'InvariantsMiner':\n",
        "      feature_extractor = preprocessing.FeatureExtractor()\n",
        "      x_train = feature_extractor.fit_transform(x_tr)\n",
        "      model = InvariantsMiner(epsilon=0.5)\n",
        "      model.fit(x_train)\n",
        "\n",
        "  elif _model == 'LogClustering':\n",
        "      feature_extractor = preprocessing.FeatureExtractor()\n",
        "      x_train = feature_extractor.fit_transform(x_tr, term_weighting='tf-idf')\n",
        "      model = LogClustering(max_dist=0.3, anomaly_threshold=0.3)\n",
        "      model.fit(x_train[y_train == 0, :]) # Use only normal samples for training\n",
        "\n",
        "  elif _model == 'IsolationForest':\n",
        "      feature_extractor = preprocessing.FeatureExtractor()\n",
        "      x_train = feature_extractor.fit_transform(x_tr)\n",
        "      model = IsolationForest(random_state=2019, max_samples=0.9999, contamination=0.03, \n",
        "                              n_jobs=4)\n",
        "      model.fit(x_train)\n",
        "\n",
        "  elif _model == 'LR':\n",
        "      feature_extractor = preprocessing.FeatureExtractor()\n",
        "      x_train = feature_extractor.fit_transform(x_tr, term_weighting='tf-idf')\n",
        "      model = LR()\n",
        "      model.fit(x_train, y_train)\n",
        "\n",
        "  elif _model == 'SVM':\n",
        "      feature_extractor = preprocessing.FeatureExtractor()\n",
        "      x_train = feature_extractor.fit_transform(x_tr, term_weighting='tf-idf')\n",
        "      model = SVM()\n",
        "      model.fit(x_train, y_train)\n",
        "\n",
        "  elif _model == 'DecisionTree':\n",
        "      feature_extractor = preprocessing.FeatureExtractor()\n",
        "      x_train = feature_extractor.fit_transform(x_tr, term_weighting='tf-idf')\n",
        "      model = DecisionTree()\n",
        "      model.fit(x_train, y_train)\n",
        "\n",
        "  x_test = feature_extractor.transform(x_te)\n",
        "  print('Train accuracy:')\n",
        "  precision, recall, f1 = model.evaluate(x_train, y_train)\n",
        "  benchmark_results.append([_model + '-train', precision, recall, f1])\n",
        "  print('Test accuracy:')\n",
        "  precision, recall, f1 = model.evaluate(x_test, y_test)\n",
        "  benchmark_results.append([_model + '-test', precision, recall, f1])\n",
        "\n",
        "  pd.DataFrame(benchmark_results, columns=['Model', 'Precision', 'Recall', 'F1']) #\\\n",
        "  #.to_csv('benchmark_result.csv', index=False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== Input data summary ======\n",
            "Loading /content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS_100k.log_structured.csv\n",
            "Warning: Only split_type=sequential is supported                 if label_file=None.\n",
            "Total: 7940 instances, train: 3970 instances, test: 3970 instances\n",
            "Evaluating PCA on HDFS:\n",
            "====== Transformed train data summary ======\n",
            "Train data shape: 3970-by-14\n",
            "\n",
            "====== Model summary ======\n",
            "n_components: 1\n",
            "Project matrix shape: 14-by-14\n",
            "SPE threshold: 14.86211234105961\n",
            "\n",
            "====== Transformed test data summary ======\n",
            "Test data shape: 3970-by-14\n",
            "\n",
            "Train accuracy:\n",
            "====== Evaluation summary ======\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-92d69121371f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train accuracy:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m   \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m   \u001b[0mbenchmark_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/CS 247/Project/loglizer/loglizer/models/PCA.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, X, y_true)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'====== Evaluation summary ======'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/CS 247/Project/loglizer/loglizer/utils.py\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mf1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0mmeasure\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                          str(average_options))\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[1;32m     80\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         raise ValueError('Expected array-like (array or non-string sequence), '\n\u001b[0;32m--> 241\u001b[0;31m                          'got %r' % y)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0msparse_pandas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'SparseSeries'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SparseArray'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected array-like (array or non-string sequence), got None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCaBh3soJLdA",
        "outputId": "76fb32d2-d9c6-41bf-bbf1-9feee9672cb1"
      },
      "source": [
        "for epoch in [2,5, 10,100]:\n",
        "  (x_train, window_y_train, y_train), (x_test, window_y_test, y_test) = dataloader.load_HDFS(struct_log, label_file=label_file, window='session', window_size=window_size, train_ratio=train_ratio, split_type='uniform')\n",
        "    \n",
        "  feature_extractor = Vectorizer()\n",
        "  train_dataset = feature_extractor.fit_transform(x_train, window_y_train, y_train)\n",
        "  test_dataset = feature_extractor.transform(x_test, window_y_test, y_test)\n",
        "\n",
        "  train_loader = Iterator(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers).iter\n",
        "  test_loader = Iterator(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers).iter\n",
        "\n",
        "  model = DeepLog(num_labels=feature_extractor.num_labels, hidden_size=hidden_size, num_directions=2, topk=topk, device=device)\n",
        "  model.fit(train_loader, epoch)\n",
        "  print(\"========\")\n",
        "  print(f\"epoch = {epoch}\")\n",
        "  print('Train validation:')\n",
        "  metrics = model.evaluate(train_loader)\n",
        "\n",
        "  print('Test validation:')\n",
        "  metrics = model.evaluate(test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== Input data summary ======\n",
            "Loading /content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS_100k.log_structured.csv\n",
            "62 251\n",
            "Slicing 1587 sessions, with window 10\n",
            "Slicing done, 8406 windows generated\n",
            "Slicing 6353 sessions, with window 10\n",
            "Slicing done, 25807 windows generated\n",
            "Train: 8406 windows (464/8406 anomaly), 7942/8406 normal\n",
            "Test: 25807 windows (842/25807 anomaly), 24965/25807 normal\n",
            "Epoch 1/2, training loss: 2.49930\n",
            "Epoch 2/2, training loss: 2.32946\n",
            "========\n",
            "epoch = 2\n",
            "Train validation:\n",
            "[('window_acc', 0.50809), ('session_acc', 0.78513), ('f1', 0.10026), ('recall', 0.30645), ('precision', 0.05994)]\n",
            "Test validation:\n",
            "[('window_acc', 0.1741), ('session_acc', 0.09586), ('f1', 0.04394), ('recall', 0.5259), ('precision', 0.02293)]\n",
            "====== Input data summary ======\n",
            "Loading /content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS_100k.log_structured.csv\n",
            "62 251\n",
            "Slicing 1587 sessions, with window 10\n",
            "Slicing done, 8406 windows generated\n",
            "Slicing 6353 sessions, with window 10\n",
            "Slicing done, 25807 windows generated\n",
            "Train: 8406 windows (464/8406 anomaly), 7942/8406 normal\n",
            "Test: 25807 windows (842/25807 anomaly), 24965/25807 normal\n",
            "Epoch 1/5, training loss: 2.49308\n",
            "Epoch 2/5, training loss: 2.34921\n",
            "Epoch 3/5, training loss: 2.16912\n",
            "Epoch 4/5, training loss: 1.90198\n",
            "Epoch 5/5, training loss: 1.59974\n",
            "========\n",
            "epoch = 5\n",
            "Train validation:\n",
            "[('window_acc', 0.50809), ('session_acc', 0.85318), ('f1', 0.10039), ('recall', 0.20968), ('precision', 0.06599)]\n",
            "Test validation:\n",
            "[('window_acc', 0.1741), ('session_acc', 0.16827), ('f1', 0.04655), ('recall', 0.51394), ('precision', 0.02438)]\n",
            "====== Input data summary ======\n",
            "Loading /content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS_100k.log_structured.csv\n",
            "62 251\n",
            "Slicing 1587 sessions, with window 10\n",
            "Slicing done, 8406 windows generated\n",
            "Slicing 6353 sessions, with window 10\n",
            "Slicing done, 25807 windows generated\n",
            "Train: 8406 windows (464/8406 anomaly), 7942/8406 normal\n",
            "Test: 25807 windows (842/25807 anomaly), 24965/25807 normal\n",
            "Epoch 1/10, training loss: 2.44087\n",
            "Epoch 2/10, training loss: 2.26711\n",
            "Epoch 3/10, training loss: 2.05650\n",
            "Epoch 4/10, training loss: 1.82681\n",
            "Epoch 5/10, training loss: 1.62058\n",
            "Epoch 6/10, training loss: 1.49100\n",
            "Epoch 7/10, training loss: 1.42772\n",
            "Epoch 8/10, training loss: 1.37037\n",
            "Epoch 9/10, training loss: 1.33163\n",
            "Epoch 10/10, training loss: 1.28269\n",
            "========\n",
            "epoch = 10\n",
            "Train validation:\n",
            "[('window_acc', 0.50809), ('session_acc', 0.85381), ('f1', 0.10078), ('recall', 0.20968), ('precision', 0.06633)]\n",
            "Test validation:\n",
            "[('window_acc', 0.1741), ('session_acc', 0.17582), ('f1', 0.04627), ('recall', 0.50598), ('precision', 0.02424)]\n",
            "====== Input data summary ======\n",
            "Loading /content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS_100k.log_structured.csv\n",
            "62 251\n",
            "Slicing 1587 sessions, with window 10\n",
            "Slicing done, 8406 windows generated\n",
            "Slicing 6353 sessions, with window 10\n",
            "Slicing done, 25807 windows generated\n",
            "Train: 8406 windows (464/8406 anomaly), 7942/8406 normal\n",
            "Test: 25807 windows (842/25807 anomaly), 24965/25807 normal\n",
            "Epoch 1/100, training loss: 2.46912\n",
            "Epoch 2/100, training loss: 2.28490\n",
            "Epoch 3/100, training loss: 2.06188\n",
            "Epoch 4/100, training loss: 1.80751\n",
            "Epoch 5/100, training loss: 1.59843\n",
            "Epoch 6/100, training loss: 1.48168\n",
            "Epoch 7/100, training loss: 1.41649\n",
            "Epoch 8/100, training loss: 1.39457\n",
            "Epoch 9/100, training loss: 1.37429\n",
            "Epoch 10/100, training loss: 1.36698\n",
            "Epoch 11/100, training loss: 1.35776\n",
            "Epoch 12/100, training loss: 1.33946\n",
            "Epoch 13/100, training loss: 1.33449\n",
            "Epoch 14/100, training loss: 1.31167\n",
            "Epoch 15/100, training loss: 1.29646\n",
            "Epoch 16/100, training loss: 1.27615\n",
            "Epoch 17/100, training loss: 1.24756\n",
            "Epoch 18/100, training loss: 1.22809\n",
            "Epoch 19/100, training loss: 1.20216\n",
            "Epoch 20/100, training loss: 1.17646\n",
            "Epoch 21/100, training loss: 1.15090\n",
            "Epoch 22/100, training loss: 1.11805\n",
            "Epoch 23/100, training loss: 1.06965\n",
            "Epoch 24/100, training loss: 1.02521\n",
            "Epoch 25/100, training loss: 0.98344\n",
            "Epoch 26/100, training loss: 0.93299\n",
            "Epoch 27/100, training loss: 0.89910\n",
            "Epoch 28/100, training loss: 0.85835\n",
            "Epoch 29/100, training loss: 0.83369\n",
            "Epoch 30/100, training loss: 0.80296\n",
            "Epoch 31/100, training loss: 0.75813\n",
            "Epoch 32/100, training loss: 0.73038\n",
            "Epoch 33/100, training loss: 0.69471\n",
            "Epoch 34/100, training loss: 0.67090\n",
            "Epoch 35/100, training loss: 0.65062\n",
            "Epoch 36/100, training loss: 0.63672\n",
            "Epoch 37/100, training loss: 0.63128\n",
            "Epoch 38/100, training loss: 0.61585\n",
            "Epoch 39/100, training loss: 0.60955\n",
            "Epoch 40/100, training loss: 0.59669\n",
            "Epoch 41/100, training loss: 0.59149\n",
            "Epoch 42/100, training loss: 0.57888\n",
            "Epoch 43/100, training loss: 0.57269\n",
            "Epoch 44/100, training loss: 0.55467\n",
            "Epoch 45/100, training loss: 0.54087\n",
            "Epoch 46/100, training loss: 0.53016\n",
            "Epoch 47/100, training loss: 0.51717\n",
            "Epoch 48/100, training loss: 0.49845\n",
            "Epoch 49/100, training loss: 0.48687\n",
            "Epoch 50/100, training loss: 0.47150\n",
            "Epoch 51/100, training loss: 0.46711\n",
            "Epoch 52/100, training loss: 0.45047\n",
            "Epoch 53/100, training loss: 0.44265\n",
            "Epoch 54/100, training loss: 0.43493\n",
            "Epoch 55/100, training loss: 0.43714\n",
            "Epoch 56/100, training loss: 0.42254\n",
            "Epoch 57/100, training loss: 0.41352\n",
            "Epoch 58/100, training loss: 0.40993\n",
            "Epoch 59/100, training loss: 0.40089\n",
            "Epoch 60/100, training loss: 0.40093\n",
            "Epoch 61/100, training loss: 0.39623\n",
            "Epoch 62/100, training loss: 0.40078\n",
            "Epoch 63/100, training loss: 0.39117\n",
            "Epoch 64/100, training loss: 0.38521\n",
            "Epoch 65/100, training loss: 0.37971\n",
            "Epoch 66/100, training loss: 0.37918\n",
            "Epoch 67/100, training loss: 0.37489\n",
            "Epoch 68/100, training loss: 0.37252\n",
            "Epoch 69/100, training loss: 0.37338\n",
            "Epoch 70/100, training loss: 0.37016\n",
            "Epoch 71/100, training loss: 0.36901\n",
            "Epoch 72/100, training loss: 0.36763\n",
            "Epoch 73/100, training loss: 0.36270\n",
            "Epoch 74/100, training loss: 0.35283\n",
            "Epoch 75/100, training loss: 0.36385\n",
            "Epoch 76/100, training loss: 0.35701\n",
            "Epoch 77/100, training loss: 0.36624\n",
            "Epoch 78/100, training loss: 0.35199\n",
            "Epoch 79/100, training loss: 0.34986\n",
            "Epoch 80/100, training loss: 0.35026\n",
            "Epoch 81/100, training loss: 0.34859\n",
            "Epoch 82/100, training loss: 0.35508\n",
            "Epoch 83/100, training loss: 0.34298\n",
            "Epoch 84/100, training loss: 0.34704\n",
            "Epoch 85/100, training loss: 0.34729\n",
            "Epoch 86/100, training loss: 0.34041\n",
            "Epoch 87/100, training loss: 0.34014\n",
            "Epoch 88/100, training loss: 0.34251\n",
            "Epoch 89/100, training loss: 0.34206\n",
            "Epoch 90/100, training loss: 0.34165\n",
            "Epoch 91/100, training loss: 0.33762\n",
            "Epoch 92/100, training loss: 0.33869\n",
            "Epoch 93/100, training loss: 0.33275\n",
            "Epoch 94/100, training loss: 0.33615\n",
            "Epoch 95/100, training loss: 0.34367\n",
            "Epoch 96/100, training loss: 0.32844\n",
            "Epoch 97/100, training loss: 0.32796\n",
            "Epoch 98/100, training loss: 0.33165\n",
            "Epoch 99/100, training loss: 0.33396\n",
            "Epoch 100/100, training loss: 0.32856\n",
            "========\n",
            "epoch = 100\n",
            "Train validation:\n",
            "[('window_acc', 0.82643), ('session_acc', 0.9603), ('f1', 0.03077), ('recall', 0.01613), ('precision', 0.33333)]\n",
            "Test validation:\n",
            "[('window_acc', 0.53838), ('session_acc', 0.95341), ('f1', 0.16854), ('recall', 0.11952), ('precision', 0.28571)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI1bt9sstW_P"
      },
      "source": [
        "for batch_size in [32,64,128,256,512,1024]:\n",
        "  (x_train, window_y_train, y_train), (x_test, window_y_test, y_test) = dataloader.load_HDFS(struct_log, label_file=label_file, window='session', window_size=window_size, train_ratio=train_ratio, split_type='uniform')\n",
        "    \n",
        "  feature_extractor = Vectorizer()\n",
        "  train_dataset = feature_extractor.fit_transform(x_train, window_y_train, y_train)\n",
        "  test_dataset = feature_extractor.transform(x_test, window_y_test, y_test)\n",
        "\n",
        "  train_loader = Iterator(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers).iter\n",
        "  test_loader = Iterator(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers).iter\n",
        "\n",
        "  model = DeepLog(num_labels=feature_extractor.num_labels, hidden_size=hidden_size, num_directions=2, topk=topk, device=device)\n",
        "  model.fit(train_loader, epoches)\n",
        "  print(\"========\")\n",
        "  print(f\"batch_size = {batch_size}\")\n",
        "  print('Train validation:')\n",
        "  metrics = model.evaluate(train_loader)\n",
        "\n",
        "  print('Test validation:')\n",
        "  metrics = model.evaluate(test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GukqzESz_2a"
      },
      "source": [
        "### N-gram (nltk)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1bZ7ylo1XH4"
      },
      "source": [
        "from itertools import product\n",
        "import math\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehibwJv83_a0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "JWhWGjcZ5GT8",
        "outputId": "283a0093-921e-4c96-f7d0-ee6e7253ce71"
      },
      "source": [
        "fake_df = pd.DataFrame({\"zip_code\": np.repeat([\"243x2\", \"2899.8\", \"5551?\", \"03754\", \"14234\"], 2)})\n",
        "fake_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>zip_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>243x2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>243x2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2899.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2899.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5551?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5551?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>03754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>03754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>14234</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  zip_code\n",
              "0    243x2\n",
              "1    243x2\n",
              "2   2899.8\n",
              "3   2899.8\n",
              "4    5551?\n",
              "5    5551?\n",
              "6    03754\n",
              "7    03754\n",
              "8    14234\n",
              "9    14234"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QifAhI8pArjH",
        "outputId": "a6cc8990-9a30-471b-e886-68d28f38b563"
      },
      "source": [
        "x_train[\"EventSequence\"].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['E5', 'E5', 'E5', 'E22', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9']),\n",
              "       list(['E5', 'E5', 'E22', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26']),\n",
              "       list(['E5', 'E22', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26', 'E26']),\n",
              "       ...,\n",
              "       list(['E5', 'E5', 'E11', 'E9', 'E11', 'E9', 'E26', 'E26', 'E11', 'E9']),\n",
              "       list(['E5', 'E11', 'E9', 'E11', 'E9', 'E26', 'E26', 'E11', 'E9', 'E26']),\n",
              "       list(['E11', 'E9', 'E11', 'E9', 'E26', 'E26', 'E11', 'E9', 'E26', 'E2'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzVjKyfp56i8",
        "outputId": "973d47aa-e5ab-460e-d654-e4456c0dcc44"
      },
      "source": [
        "from itertools import groupby\n",
        "\n",
        "def get_pattern(value):\n",
        "    pattern = ''\n",
        "    value = list(str(value))\n",
        "    for c in value:\n",
        "        if c.isnumeric():\n",
        "            pattern += 'd'\n",
        "        elif c.isalpha():\n",
        "            pattern += 'l'\n",
        "        else:\n",
        "            pattern += 's'\n",
        "    \n",
        "    grouped_pattern = [''.join(g) for _, g in groupby(pattern)]\n",
        "    \n",
        "    return ''.join([f'{v[0]}({len(v)})' for v in grouped_pattern])\n",
        "\n",
        "zip_codes = fake_df['zip_code'].values\n",
        "zip_codes_pattern = [get_pattern(zc) for zc in zip_codes]\n",
        "zip_codes_pattern"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['d(3)l(1)d(1)',\n",
              " 'd(3)l(1)d(1)',\n",
              " 'd(4)s(1)d(1)',\n",
              " 'd(4)s(1)d(1)',\n",
              " 'd(4)s(1)',\n",
              " 'd(4)s(1)',\n",
              " 'd(5)',\n",
              " 'd(5)',\n",
              " 'd(5)',\n",
              " 'd(5)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "bvRwdW9x-SYt",
        "outputId": "19bb4db6-1ae0-4a0e-9885-7589cbe6b25e"
      },
      "source": [
        "#!pip install \"nltk==3.6.2\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     || 1.5MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (2019.12.20)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "_7ct6jHPHu_d",
        "outputId": "ccbc1c66-6f53-4c4f-f0ef-1d6500a2d2b5"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SessionId</th>\n",
              "      <th>EventSequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[E5, E5, E5, E22, E11, E9, E11, E9, E11, E9]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[E5, E5, E22, E11, E9, E11, E9, E11, E9, E26]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>[E5, E22, E11, E9, E11, E9, E11, E9, E26, E26]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>[E22, E11, E9, E11, E9, E11, E9, E26, E26, E26]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>[E11, E9, E11, E9, E11, E9, E26, E26, E26, E2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8401</th>\n",
              "      <td>1586</td>\n",
              "      <td>[E5, E22, E5, E5, E11, E9, E11, E9, E26, E26]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8402</th>\n",
              "      <td>1586</td>\n",
              "      <td>[E22, E5, E5, E11, E9, E11, E9, E26, E26, E11]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8403</th>\n",
              "      <td>1586</td>\n",
              "      <td>[E5, E5, E11, E9, E11, E9, E26, E26, E11, E9]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8404</th>\n",
              "      <td>1586</td>\n",
              "      <td>[E5, E11, E9, E11, E9, E26, E26, E11, E9, E26]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8405</th>\n",
              "      <td>1586</td>\n",
              "      <td>[E11, E9, E11, E9, E26, E26, E11, E9, E26, E2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8406 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      SessionId                                    EventSequence\n",
              "0             0     [E5, E5, E5, E22, E11, E9, E11, E9, E11, E9]\n",
              "1             0    [E5, E5, E22, E11, E9, E11, E9, E11, E9, E26]\n",
              "2             0   [E5, E22, E11, E9, E11, E9, E11, E9, E26, E26]\n",
              "3             0  [E22, E11, E9, E11, E9, E11, E9, E26, E26, E26]\n",
              "4             0   [E11, E9, E11, E9, E11, E9, E26, E26, E26, E2]\n",
              "...         ...                                              ...\n",
              "8401       1586    [E5, E22, E5, E5, E11, E9, E11, E9, E26, E26]\n",
              "8402       1586   [E22, E5, E5, E11, E9, E11, E9, E26, E26, E11]\n",
              "8403       1586    [E5, E5, E11, E9, E11, E9, E26, E26, E11, E9]\n",
              "8404       1586   [E5, E11, E9, E11, E9, E26, E26, E11, E9, E26]\n",
              "8405       1586   [E11, E9, E11, E9, E26, E26, E11, E9, E26, E2]\n",
              "\n",
              "[8406 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwEMYGD5HWQW",
        "outputId": "feb50b87-8b23-4d37-d101-33035f58f6d0"
      },
      "source": [
        "#x_train\n",
        "#token= x_train.EventSequence[0]\n",
        "list(zip(*[token[i:] for i in range(3)]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('E5', 'E5', 'E5'),\n",
              " ('E5', 'E5', 'E22'),\n",
              " ('E5', 'E22', 'E11'),\n",
              " ('E22', 'E11', 'E9'),\n",
              " ('E11', 'E9', 'E11'),\n",
              " ('E9', 'E11', 'E9'),\n",
              " ('E11', 'E9', 'E11'),\n",
              " ('E9', 'E11', 'E9'),\n",
              " ('E5', 'E5', 'E5'),\n",
              " ('E5', 'E5', 'E22'),\n",
              " ('E5', 'E22', 'E11'),\n",
              " ('E22', 'E11', 'E9'),\n",
              " ('E11', 'E9', 'E11'),\n",
              " ('E9', 'E11', 'E9'),\n",
              " ('E11', 'E9', 'E11'),\n",
              " ('E9', 'E11', 'E9')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz50uKlXIAAW"
      },
      "source": [
        "def ngram_preprocess(x_train, n):\n",
        "  All_Sess = set(x_train.SessionId)\n",
        "  Sess_ngrams = []\n",
        "  for id in All_Sess:\n",
        "    Sess_ngrams.append(list(zip(*[token[i:] for i in range(3)])))\n",
        "  pad_seqs_pttrn = [pad_both_ends(seq_p, n) for seq_p in All_pattrns]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdPSht2_Ea2W"
      },
      "source": [
        "def get_pattern(sess_seqs):\n",
        "  return [''.join(seq) for seq in sess_seqs]\n",
        "\n",
        "def ngram_preprocess(x_train, n):\n",
        "  All_Sess = set(x_train.SessionId)\n",
        "  All_pattrns = []\n",
        "  for id in All_Sess:\n",
        "    All_seqs = x_train[x_train.SessionId == id].EventSequence.values\n",
        "    seq_pttrn = get_pattern(All_seqs)\n",
        "    All_pattrns += seq_pttrn\n",
        "\n",
        "  pad_seqs_pttrn = [pad_both_ends(seq_p, n) for seq_p in All_pattrns]\n",
        "  my_ngrams = [ngrams(psp, n=n) for psp in pad_seqs_pttrn]\n",
        "  processed_d = {\"tokens\": my_ngrams}\n",
        "  return processed_d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-c29yJy0SKK"
      },
      "source": [
        "def get_pattern(sess_seqs):\n",
        "  return [''.join(seq) for seq in sess_seqs]\n",
        "\n",
        "def ngram_preprocess(x_train, n):\n",
        "  All_Sess = set(x_train.SessionId)\n",
        "  All_pattrns = []\n",
        "  for id in All_Sess:\n",
        "    All_seqs = x_train[x_train.SessionId == id].EventSequence.values\n",
        "    seq_pttrn = get_pattern(All_seqs)\n",
        "    All_pattrns += seq_pttrn\n",
        "\n",
        "  pad_seqs_pttrn = [pad_both_ends(seq_p, n) for seq_p in All_pattrns]\n",
        "  my_ngrams = [ngrams(psp, n=n) for psp in pad_seqs_pttrn]\n",
        "  vocab = list(flatten(pad_both_ends(seq_p, n=n-1) for seq_p in All_pattrns))\n",
        "  processed_d = {\"ngram\": my_ngrams, \"vocab\": vocab}\n",
        "  return processed_d\n",
        "\n",
        "\n",
        "class N_GramModel(object):\n",
        "   def __init__(self, train_data, n, laplace=1):\n",
        "        self.n = n\n",
        "        self.laplace = laplace\n",
        "        self.prcs_dict = ngram_preprocess(train_data, n)\n",
        "        self.tokens = self.prcs_dict[\"tokens\"]\n",
        "        self.vocab  = nltk.FreqDist(self.tokens)\n",
        "        self.model  = self._create_model()\n",
        "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
        "\n",
        "  def _smooth(self):\n",
        "        \"\"\"Apply Laplace smoothing to n-gram frequency distribution.\n",
        "        \n",
        "        Here, n_grams refers to the n-grams of the tokens in the training corpus,\n",
        "        while m_grams refers to the first (n-1) tokens of each n-gram.\n",
        "        Returns:\n",
        "            dict: Mapping of each n-gram (tuple of str) to its Laplace-smoothed \n",
        "            probability (float).\n",
        "        \"\"\"\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
        "        n_vocab = nltk.FreqDist(n_grams)\n",
        "\n",
        "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
        "        m_vocab = nltk.FreqDist(m_grams)\n",
        "\n",
        "        def smoothed_count(n_gram, n_count):\n",
        "            m_gram = n_gram[:-1]\n",
        "            m_count = m_vocab[m_gram]\n",
        "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
        "\n",
        "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
        "\n",
        "    def _create_model(self):\n",
        "        \"\"\"Create a probability distribution for the vocabulary of the training corpus.\n",
        "        \n",
        "        If building a unigram model, the probabilities are simple relative frequencies\n",
        "        of each token with the entire corpus.\n",
        "        Otherwise, the probabilities are Laplace-smoothed relative frequencies.\n",
        "        Returns:\n",
        "            A dict mapping each n-gram (tuple of str) to its probability (float).\n",
        "        \"\"\"\n",
        "        if self.n == 1:\n",
        "            num_tokens = len(self.tokens)\n",
        "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
        "        else:\n",
        "            return self._smooth()\n",
        "\n",
        "    def _convert_oov(self, ngram):\n",
        "        \"\"\"Convert, if necessary, a given n-gram to one which is known by the model.\n",
        "        Starting with the unmodified ngram, check each possible permutation of the n-gram\n",
        "        with each index of the n-gram containing either the original token or <UNK>. Stop\n",
        "        when the model contains an entry for that permutation.\n",
        "        This is achieved by creating a 'bitmask' for the n-gram tuple, and swapping out\n",
        "        each flagged token for <UNK>. Thus, in the worst case, this function checks 2^n\n",
        "        possible n-grams before returning.\n",
        "        Returns:\n",
        "            The n-gram with <UNK> tokens in certain positions such that the model\n",
        "            contains an entry for it.\n",
        "        \"\"\"\n",
        "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
        "\n",
        "        ngram = (ngram,) if type(ngram) is str else ngram\n",
        "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
        "            if possible_known in self.model:\n",
        "                return possible_known\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4ZbBjwOU3Zz"
      },
      "source": [
        "### Log_clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4lmrrxcVzcQ"
      },
      "source": [
        "from loglizer.models import *\n",
        "from loglizer import dataloader, preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDdV4V-tVPW4",
        "outputId": "21d7cd38-a083-4c61-fe46-f2f4fc80e007"
      },
      "source": [
        "max_dist = 0.3 # the threshold to stop the clustering process\n",
        "anomaly_threshold = 0.3 # the threshold for anomaly detection\n",
        "train_ratio = 0.2\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = dataloader.load_HDFS(struct_log,\n",
        "                                                            label_file=label_file,\n",
        "                                                            window='session', \n",
        "                                                            train_ratio=train_ratio,\n",
        "                                                            split_type='uniform')\n",
        "feature_extractor = preprocessing.FeatureExtractor()\n",
        "x_train = feature_extractor.fit_transform(X_train, term_weighting='tf-idf')\n",
        "x_test = feature_extractor.transform(X_test)\n",
        "\n",
        "model = LogClustering(max_dist=max_dist, anomaly_threshold=anomaly_threshold)\n",
        "model.fit(x_train[y_train == 0, :]) # Use only normal samples for training\n",
        "\n",
        "print('Train validation:')\n",
        "precision, recall, f1 = model.evaluate(x_train, y_train)\n",
        "\n",
        "print('Test validation:')\n",
        "precision, recall, f1 = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== Input data summary ======\n",
            "Loading /content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS_100k.log_structured.csv\n",
            "62 251\n",
            "Total: 7940 instances, 313 anomaly, 7627 normal\n",
            "Train: 1587 instances, 62 anomaly, 1525 normal\n",
            "Test: 6353 instances, 251 anomaly, 6102 normal\n",
            "\n",
            "====== Transformed train data summary ======\n",
            "Train data shape: 1587-by-14\n",
            "\n",
            "====== Transformed test data summary ======\n",
            "Test data shape: 6353-by-14\n",
            "\n",
            "====== Model summary ======\n",
            "Starting offline clustering...\n",
            "Processed 1000 instances.\n",
            "Found 3 clusters offline.\n",
            "\n",
            "Starting online clustering...\n",
            "Processed 1525 instances.\n",
            "Found 3 clusters online.\n",
            "\n",
            "Train validation:\n",
            "====== Evaluation summary ======\n",
            "Precision: 1.000, recall: 0.081, F1-measure: 0.149\n",
            "\n",
            "Test validation:\n",
            "====== Evaluation summary ======\n",
            "Precision: 0.979, recall: 0.554, F1-measure: 0.707\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4FMtItBWCTq",
        "outputId": "feb2029b-0a76-4bdb-954e-87f4cdc028bb"
      },
      "source": [
        "model.representatives"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-7.55862040e-12, -2.51954013e-12,  4.18629145e-02,  4.26294917e-02,\n",
              "         4.26294917e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00]),\n",
              " array([-7.55862040e-12, -2.51954013e-12,  4.18629145e-02,  4.26294917e-02,\n",
              "         4.26294917e-02,  1.80260280e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00]),\n",
              " array([-1.63770109e-11, -2.51954013e-12,  4.18629145e-02,  4.26294917e-02,\n",
              "         9.23638987e-02,  1.14186227e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  2.90019430e+01,  1.65725389e+01,  1.65725389e+01,\n",
              "         1.65725389e+01,  7.33141030e+02])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3n6MtgvVDY2"
      },
      "source": [
        "### Embedding based on event as word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orSSo1xtfN2g",
        "outputId": "18b59d42-4854-448e-c207-ca7e7e35e0c9"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = dataloader.load_HDFS(struct_log,\n",
        "                                                            label_file=label_file,\n",
        "                                                            window='session', \n",
        "                                                            train_ratio=train_ratio,\n",
        "                                                            split_type='uniform')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== Input data summary ======\n",
            "Loading /content/drive/MyDrive/CS 247/Project/loglizer/data/HDFS/HDFS_100k.log_structured.csv\n",
            "62 251\n",
            "Total: 7940 instances, 313 anomaly, 7627 normal\n",
            "Train: 1587 instances, 62 anomaly, 1525 normal\n",
            "Test: 6353 instances, 251 anomaly, 6102 normal\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCZf--CNfXA4",
        "outputId": "511a7c7a-233d-4187-aa64-85b9936bf6f8"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['E5',\n",
              " 'E22',\n",
              " 'E5',\n",
              " 'E5',\n",
              " 'E11',\n",
              " 'E11',\n",
              " 'E9',\n",
              " 'E9',\n",
              " 'E26',\n",
              " 'E11',\n",
              " 'E9',\n",
              " 'E26',\n",
              " 'E26',\n",
              " 'E2',\n",
              " 'E2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-Rx86LNnYHV",
        "outputId": "d0c1bac7-9ddd-4a65-e5ee-cb9648adc0a9"
      },
      "source": [
        "# extremely long / short seq -> system error\n",
        "for i,_ in enumerate(X_train):\n",
        "  if len(_) < 12 or len(_) > 16:\n",
        "    print(i, len(_), y_train[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103 216 1\n",
            "251 2 1\n",
            "350 4 1\n",
            "427 8 1\n",
            "752 2 1\n",
            "1276 2 1\n",
            "1575 249 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NutpRLGifPKW"
      },
      "source": [
        "# import necessary libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange \n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sb\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from scipy.linalg import sqrtm \n",
        "from numpy import linalg as LA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8VOgonsf0Qb"
      },
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, size, embed_dim=8):\n",
        "        super(Embedding, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.nodes_embeddings = nn.Embedding(size, embed_dim)\n",
        "\n",
        "        # Initialization\n",
        "        self.nodes_embeddings.weight.data = self.nodes_embeddings.weight.data.uniform_(-.5, .5) / embed_dim\n",
        "\n",
        "    def loss(self, v_i, v_j, negsamples):\n",
        "        \n",
        "        v_i = self.nodes_embeddings(v_i) \n",
        "        v_j = self.nodes_embeddings(v_j) \n",
        "        negsamples = -1 * self.nodes_embeddings(negsamples)\n",
        "        pos_pair_loss = torch.mul(v_i, v_j) \n",
        "        pos_pair_loss = F.logsigmoid(torch.sum(pos_pair_loss, dim=1)) \n",
        "\n",
        "        neg_pair_loss = torch.mul(v_i.view(-1, 1, self.embed_dim), negsamples)\n",
        "        neg_pair_loss = torch.sum(neg_pair_loss, dim=2) \n",
        "        neg_pair_loss = torch.sum(F.logsigmoid(neg_pair_loss),dim=1) \n",
        "        \n",
        "        contrastive_loss = pos_pair_loss + neg_pair_loss \n",
        "\n",
        "        return torch.mean(contrastive_loss) * -1\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9NSKRkFghrD"
      },
      "source": [
        "#     generating batches of data.\n",
        "\n",
        "def makeData(samplededges, negsamplesize, degree):\n",
        "    sampledNodes = set()\n",
        "    nodesProb = []\n",
        "    sumofDegree = 0\n",
        "    for e in samplededges:\n",
        "        sampledNodes.add(e[0])\n",
        "        sampledNodes.add(e[1])\n",
        "    sampledNodes = list(sampledNodes)\n",
        "    nodesProb = [pow(degree[v],3/4) for v in sampledNodes]\n",
        "    sumofDegree = sum(nodesProb)\n",
        "    nodesProb[:] = [x/sumofDegree for x in nodesProb]\n",
        "\n",
        "    for e in samplededges:\n",
        "        sourcenode, targetnode = e[0], e[1]\n",
        "        negnodes = []\n",
        "        negsamples = 0\n",
        "        while negsamples < negsamplesize:\n",
        "            samplednode = np.random.choice(sampledNodes, p = nodesProb)\n",
        "            \n",
        "            if (samplednode == sourcenode) or (samplednode == targetnode):\n",
        "                continue\n",
        "            else:\n",
        "                negsamples += 1\n",
        "                negnodes += [samplednode]\n",
        "        yield [e[0], e[1]] + negnodes\n",
        "\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqCE4g9rhGhH"
      },
      "source": [
        "def get_degree(X_train, window_size):\n",
        "  nodes = set()\n",
        "  nodes_d = {}\n",
        "  for seq in X_train:\n",
        "    if window_size == 1:\n",
        "      for event in seq:\n",
        "        nodes.add(event)\n",
        "        nodes_d[event] = nodes_d.get(event, 0) + 1\n",
        "    else:\n",
        "      m = len(seq)\n",
        "      for i in range(m+1 - window_size):\n",
        "        event = tuple(seq[i:i+window_size])\n",
        "        nodes.add(event)\n",
        "        nodes_d[event] = nodes_d.get(event, 0) + 1\n",
        "  return nodes_d \n",
        "\n",
        "def create_edges(samples):\n",
        "  edges = []\n",
        "  for seq in samples:\n",
        "    m = len(seq)\n",
        "    for i in range(m-1):\n",
        "      edges.append([seq[i], seq[i+1]])\n",
        "  return edges"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsmlP8TPncTH"
      },
      "source": [
        "n_epochs = 100\n",
        "neg_size = 5\n",
        "n_batches = 3\n",
        "batch_size = 5\n",
        "degree = get_degree(X_train, window_size= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyAjxOecnlIQ"
      },
      "source": [
        "node_to_ind = {x:i for i,x in enumerate(degree.keys())}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMctzFwKgjHW",
        "outputId": "a8e1443f-cbf5-4cb2-ca22-771420ad5d18"
      },
      "source": [
        "# training\n",
        "\n",
        "embed = Embedding(len(degree), embed_dim=8)\n",
        "opt = optim.Adam(embed.parameters())\n",
        " \n",
        "for epoch in range(n_epochs):\n",
        "    for b in trange(n_batches):\n",
        "        opt.zero_grad()\n",
        "        edge_idx = np.random.choice(len(X_train), batch_size)\n",
        "        samplededges = create_edges(X_train[edge_idx])\n",
        "        \n",
        "        batch = list(makeData(samplededges, neg_size, degree))\n",
        "        recoded_batch = []\n",
        "        for seq in batch:\n",
        "          recoded_batch.append([node_to_ind[e] for e in seq])\n",
        "\n",
        "        batch = torch.LongTensor(recoded_batch)\n",
        "        \n",
        "        # based on the generated batch, train LINE via minimizing the loss.\n",
        "        v_i = batch[:,0]\n",
        "        v_j = batch[:,1]\n",
        "        negsamples =  batch[:,2:]\n",
        "        loss = embed.loss(v_i, v_j, negsamples)\n",
        "        loss.backward()\n",
        "        opt.step()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 3/3 [00:00<00:00, 64.65it/s]\n",
            "100%|| 3/3 [00:00<00:00, 58.00it/s]\n",
            "100%|| 3/3 [00:00<00:00, 58.94it/s]\n",
            "100%|| 3/3 [00:00<00:00, 44.94it/s]\n",
            "100%|| 3/3 [00:00<00:00, 50.67it/s]\n",
            "100%|| 3/3 [00:00<00:00, 55.50it/s]\n",
            "100%|| 3/3 [00:00<00:00, 63.62it/s]\n",
            "100%|| 3/3 [00:00<00:00, 53.62it/s]\n",
            "100%|| 3/3 [00:00<00:00, 66.97it/s]\n",
            "100%|| 3/3 [00:00<00:00, 45.02it/s]\n",
            "100%|| 3/3 [00:00<00:00, 49.03it/s]\n",
            "100%|| 3/3 [00:00<00:00, 42.27it/s]\n",
            "100%|| 3/3 [00:00<00:00, 47.85it/s]\n",
            "100%|| 3/3 [00:00<00:00, 48.62it/s]\n",
            "100%|| 3/3 [00:00<00:00, 45.06it/s]\n",
            "100%|| 3/3 [00:00<00:00, 45.48it/s]\n",
            "100%|| 3/3 [00:00<00:00, 51.54it/s]\n",
            "100%|| 3/3 [00:00<00:00, 64.55it/s]\n",
            "100%|| 3/3 [00:00<00:00, 61.45it/s]\n",
            "100%|| 3/3 [00:00<00:00, 70.77it/s]\n",
            "100%|| 3/3 [00:00<00:00, 60.57it/s]\n",
            "100%|| 3/3 [00:00<00:00, 64.18it/s]\n",
            "100%|| 3/3 [00:00<00:00, 55.22it/s]\n",
            "100%|| 3/3 [00:00<00:00, 40.85it/s]\n",
            "100%|| 3/3 [00:00<00:00, 47.88it/s]\n",
            "100%|| 3/3 [00:00<00:00, 46.92it/s]\n",
            "100%|| 3/3 [00:00<00:00, 38.45it/s]\n",
            "100%|| 3/3 [00:00<00:00, 46.27it/s]\n",
            "100%|| 3/3 [00:00<00:00, 44.36it/s]\n",
            "100%|| 3/3 [00:00<00:00, 46.57it/s]\n",
            "100%|| 3/3 [00:00<00:00, 38.89it/s]\n",
            "100%|| 3/3 [00:00<00:00, 66.73it/s]\n",
            "100%|| 3/3 [00:00<00:00, 54.60it/s]\n",
            "100%|| 3/3 [00:00<00:00, 73.46it/s]\n",
            "100%|| 3/3 [00:00<00:00, 66.28it/s]\n",
            "100%|| 3/3 [00:00<00:00, 68.69it/s]\n",
            "100%|| 3/3 [00:00<00:00, 50.98it/s]\n",
            "100%|| 3/3 [00:00<00:00, 52.03it/s]\n",
            "100%|| 3/3 [00:00<00:00, 47.33it/s]\n",
            "100%|| 3/3 [00:00<00:00, 54.46it/s]\n",
            "100%|| 3/3 [00:00<00:00, 61.49it/s]\n",
            "100%|| 3/3 [00:00<00:00, 64.57it/s]\n",
            "100%|| 3/3 [00:00<00:00, 58.53it/s]\n",
            "100%|| 3/3 [00:00<00:00, 57.95it/s]\n",
            "100%|| 3/3 [00:00<00:00, 49.70it/s]\n",
            "100%|| 3/3 [00:00<00:00, 41.38it/s]\n",
            "100%|| 3/3 [00:00<00:00, 42.52it/s]\n",
            "100%|| 3/3 [00:00<00:00, 65.41it/s]\n",
            "100%|| 3/3 [00:00<00:00, 59.27it/s]\n",
            "100%|| 3/3 [00:00<00:00, 63.76it/s]\n",
            "100%|| 3/3 [00:00<00:00, 52.92it/s]\n",
            "100%|| 3/3 [00:00<00:00, 54.52it/s]\n",
            "100%|| 3/3 [00:00<00:00, 68.38it/s]\n",
            "100%|| 3/3 [00:00<00:00, 49.96it/s]\n",
            "100%|| 3/3 [00:00<00:00, 54.22it/s]\n",
            "100%|| 3/3 [00:00<00:00, 21.67it/s]\n",
            "100%|| 3/3 [00:00<00:00, 55.72it/s]\n",
            "100%|| 3/3 [00:00<00:00, 52.92it/s]\n",
            "100%|| 3/3 [00:00<00:00, 31.85it/s]\n",
            "100%|| 3/3 [00:00<00:00, 55.81it/s]\n",
            "100%|| 3/3 [00:00<00:00, 59.01it/s]\n",
            "100%|| 3/3 [00:00<00:00, 56.80it/s]\n",
            "100%|| 3/3 [00:00<00:00, 53.58it/s]\n",
            "100%|| 3/3 [00:00<00:00, 51.95it/s]\n",
            "100%|| 3/3 [00:00<00:00, 64.13it/s]\n",
            "100%|| 3/3 [00:00<00:00, 30.53it/s]\n",
            "100%|| 3/3 [00:00<00:00, 46.98it/s]\n",
            "100%|| 3/3 [00:00<00:00, 51.05it/s]\n",
            "100%|| 3/3 [00:00<00:00, 52.14it/s]\n",
            "100%|| 3/3 [00:00<00:00, 56.01it/s]\n",
            "100%|| 3/3 [00:00<00:00, 53.47it/s]\n",
            "100%|| 3/3 [00:00<00:00, 61.77it/s]\n",
            "100%|| 3/3 [00:00<00:00, 49.81it/s]\n",
            "100%|| 3/3 [00:00<00:00, 59.00it/s]\n",
            "100%|| 3/3 [00:00<00:00, 63.49it/s]\n",
            "100%|| 3/3 [00:00<00:00, 64.65it/s]\n",
            "100%|| 3/3 [00:00<00:00, 61.52it/s]\n",
            "100%|| 3/3 [00:00<00:00, 28.66it/s]\n",
            "100%|| 3/3 [00:00<00:00, 62.50it/s]\n",
            "100%|| 3/3 [00:00<00:00, 47.13it/s]\n",
            "100%|| 3/3 [00:00<00:00, 65.60it/s]\n",
            "100%|| 3/3 [00:00<00:00, 58.96it/s]\n",
            "100%|| 3/3 [00:00<00:00, 55.63it/s]\n",
            "100%|| 3/3 [00:00<00:00, 50.60it/s]\n",
            "100%|| 3/3 [00:00<00:00, 46.33it/s]\n",
            "100%|| 3/3 [00:00<00:00, 62.72it/s]\n",
            "100%|| 3/3 [00:00<00:00, 52.28it/s]\n",
            "100%|| 3/3 [00:00<00:00, 54.36it/s]\n",
            "100%|| 3/3 [00:00<00:00, 50.23it/s]\n",
            "100%|| 3/3 [00:00<00:00, 57.64it/s]\n",
            "100%|| 3/3 [00:00<00:00, 48.55it/s]\n",
            "100%|| 3/3 [00:00<00:00, 43.28it/s]\n",
            "100%|| 3/3 [00:00<00:00, 26.61it/s]\n",
            "100%|| 3/3 [00:00<00:00, 45.59it/s]\n",
            "100%|| 3/3 [00:00<00:00, 55.69it/s]\n",
            "100%|| 3/3 [00:00<00:00, 44.31it/s]\n",
            "100%|| 3/3 [00:00<00:00, 53.65it/s]\n",
            "100%|| 3/3 [00:00<00:00, 61.58it/s]\n",
            "100%|| 3/3 [00:00<00:00, 62.22it/s]\n",
            "100%|| 3/3 [00:00<00:00, 50.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "oAguBR0ApTr9",
        "outputId": "5871f837-fd53-46bd-b6b9-90359b6bb95d"
      },
      "source": [
        "# TSNE visualization, with node id on\n",
        "\n",
        "emb  = embed.nodes_embeddings.weight.data.numpy()\n",
        "tsne_emb = TSNE(n_components = 2, perplexity = 5, learning_rate = 10).fit_transform(emb)\n",
        "\n",
        "plt.scatter(tsne_emb[:,0], tsne_emb[:,1])\n",
        "for i in range(len(tsne_emb)):\n",
        "    plt.annotate(str(i), xy=(tsne_emb[i,0], tsne_emb[i,1]))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZOklEQVR4nO3df3RU9Z3/8eebABoUjCwJkkTBHxggiQbIot9vu7GIIdRSfkSlWtyFgoftOW1Pe/ptXDh+e1zbZUnBrtDT7nLwF7R+j3y1pYFFDFrE0m3rlwbDj4BNoUI3GSLEHxGF8CPh8/0jk3QyJjJJZubO5L4e58xh5s5k7ivX8XVuPnPv55pzDhER8YcBXgcQEZH4UemLiPiISl9ExEdU+iIiPqLSFxHxkYFeBwg1YsQIN2bMGK9jiIgklT179rzrnEuP5LUJVfpjxoyhqqrK6xgiIknFzP4S6Ws1vCMi4iMq/X6mtbWViRMnMnPmTK+jiEgCUun3M2vWrGH8+PFexxCRBKXS70fq6+t56aWXeOihh7yOIiIJSqXfj3zrW99i5cqVDBig/6wi0rWEOnpHeqaiOsCq7bUcb2rm8oa9jBlwBZMnT+b111/3OpqIJCiVfpKqqA6wbNMBmi+0AnD8T3s5cnAnGZnXMuDiBU6dOsWDDz7Ic88953FSEUkkKv0ktWp7bUfhA1x9x0KuvmMhWWmpLL99AI8//rgKX0Q+QYO/Sep4U3OPlouIgEo/aWWmpXa7/HOf+xxbt26Nc6LEsGjRIjIyMsjLy+tYVlZWxrhx47jllluYO3cuTU1NHiYU8ZZKP0mVleSQOiil07LUQSmUleR4lCgxLFy4kMrKyk7LiouLqampYf/+/dx8882sWLHCo3Qi3lPpJ6k5E7NYUZpPVloqBmSlpbKiNJ85E7O8juapoqIihg8f3mnZ9OnTGTiw7eur22+/nfr6ei+iiSQEfZGbxOZMzPJ9yffUM888w5e+9CWvY4h4RqUvSS/0fIXMtFQW5A/p8nXLly9n4MCBzJ8/P84JRRKHSl+SWvj5CoGmZn5QWceFsy2dXrd+/Xq2bt3Kjh07MDMvoookBJW+JLXw8xUAzrW08v7H5zoeV1ZWsnLlSn79618zZEjXfwWI+IVKX5Ja+HkJjVtWcu6/D9DafIrs7Gwee+wxVqxYwblz5yguLgbavsxdu3atF3FFPKfSl6SWmZZKIKT402c9DLQdzfTbpXcCsHjxYk+yhXriiSd46qmnMDPy8/N59tlnufzyy72OJT4UlUM2zewZMztpZjUhy4ab2atmdjj479XRWJdIqGQ4XyEQCPCjH/2IqqoqampqaG1tZePGjV7HEp+K1nH664EZYcuWAjucc2OBHcHHIlGVLOcrtLS00NzcTEtLC2fOnCEzM9PrSOJTURnecc7tMrMxYYtnA58L3t8AvA78UzTWJxIq0c9XyMrK4jvf+Q7XXXcdqampTJ8+nenTp3sdS3wqlmP6I51zDcH77wAju3qRmS0BlgBcd911MYwjEj+h5w5kXNbCmW0vcPToUdLS0rjvvvt47rnnePDBB72OKT4Ul2kYnHMOcN08t845V+icK0xPT49HHJGYaj93INDUjAPe3vcGdReu5Lf15xk0aBClpaX87ne/8zqm+FQsS/+EmY0CCP57MobrEkkY4ecODByWzpn6P1L+n/twzrFjxw5dvF48E8vS3wIsCN5fAGyO4bpEEkb4uQOXZeYwJOcz7Fnzj+Tn53Px4kWWLFniUTrxu6iM6ZvZ87R9aTvCzOqBR4Fy4AUzWwz8BZgXjXWJJLrwcwcA0v5uPrlffKjj3AERr0Tr6J0HunlqWjTeXySZlJXkdJoPCBLv3AHxL52RKxJl7YePhs78WVaSk9CHlYp/qPRFYiDRzx0Q/9KVs0REfESlLyLiIyp9EREfUemLiPiISl9ExEdU+iIiPqLSFxHxEZW+iIiPqPRFRHxEpS8i4iMqfRERH1Hpi4j4iEpfRMRHVPoiIj6i0hcR8RGVvoiIj6j0RUR8RKUvIuIjKn0RER9R6YvvLVq0iIyMDPLy8jqWffe73+WWW26hoKCA6dOnc/z4cQ8TikSPSl98b+HChVRWVnZaVlZWxv79+9m7dy8zZ87ke9/7nkfpRKJLpS++V1RUxPDhwzstGzZsWMf906dPY2bxjiUSEwO9DiCSqB555BF++tOfctVVV7Fz506v44hEhfb0JSJdjXu///77FBcXM3bsWIqLi/nggw88TBh9y5cvp66ujvnz5/PjH//Y6zgiUaHSl4h0Ne5dXl7OtGnTOHz4MNOmTaO8vNyjdD1XUR3gM+Wvcf3Sl/hM+Wu8cvCdbl87f/58fvGLX8QxnUjsqPQlIl2Ne2/evJkFCxYAsGDBAioqKryI1mMV1QGWbTpAoKkZBwSamvlBZS2nzrZ0vObw4cMd9zdv3sy4ceM8SCoSfRrTl147ceIEo0aNAuCaa67hxIkTHieKzKrttTRfaO143LhlJef++wAXm0+RnZ3NY489xrZt26itrWXAgAGMHj2atWvXephYJHpU+hIVZpY0R7gcb2ru9Dh91sMAGHC0/AsALF68ON6xROJCpS/dqqgOsGp7LcebmslMS2VB/pBOz48cOZKGhgZGjRpFQ0MDGRkZHiXtmcy0VAJhxd++XKS/05i+dCmSce9Zs2axYcMGADZs2MDs2bM9StszZSU5pA5K6bQsdVAKZSU5HiUSiZ+Y7+mb2THgI6AVaHHOFcZ6ndJ3kYx7L126lHnz5vH0008zevRoXnjhBQ8TR27OxCyATn/FlJXkdCwX6c/MORfbFbSVfqFz7t1LvbawsNBVVVXFNI9E5vqlL9HVJyN03FtEEoOZ7Yl0h1rDO9Kl7sa3Ne4tktziUfoOeMXM9pjZkvAnzWyJmVWZWVVjY2Mc4kgkNO4t0j/F4+idzzrnAmaWAbxqZn90zu1qf9I5tw5YB23DO3HIIxHQuLdI/xTz0nfOBYL/njSzXwJTgF2f/lOSCOZMzFLJi/QzMR3eMbMrzGxo+31gOlATy3WKiEj3Yj2mPxL4LzPbB+wGXnLOVV7iZ0R8q6vZTF988UVyc3MZMGAAOrpN+iqmpe+ce9s5d2vwluucWx7L9Ykku65mM83Ly2PTpk0UFRV5lEr6E03DIJJAioqKOHbsWKdl48eP9yaM9Es6Tl9ExEdU+iIiPqLhHRGPXWo2U5FoUumLeKh9NtP2ye3aZjOt40LIbKYi0aTSF/FQJLOZDh8+nG984xs0NjbyhS98gYKCArZv3+5haklmKn0RD0VyFS+AuXPnxjOW9GP6IlfEQ5rNVOJNpS/iIc1mKvGm4R0RD2k2U4k3lb6IxzSbqcSThndERHxEpS9xV1tbS0FBQcdt2LBhrF692utYIr6g4R2Ju5ycHPbu3QtAa2srWVlZOiRRJE60py+e2rFjBzfeeCOjR4/2OoqIL6j0xVMbN27kgQce8DqGiG9oeEfiInxSsbKSHO7OTWfLli2sWLHC63givqHSl5jralKxZZsO8P9eP86kSZMYOXKkxwlF/EPDOxJz4ZOKATRfaOXJ9T/T0I5InGlPX2IufFIxgIvnz/LBn/ZQWlrhQSIR/9KevsRcV5OHDRh8Obc/WsFVV13lQSIR/1LpS8xpUjGRxKHhHYk5TSomkjhU+hIXmlRMJDFoeEdExEdU+iLSY01NTdx7772MGzeO8ePH8/vf/97rSBIhDe+ISI9985vfZMaMGfz85z/n/PnznDlzxutIEiGVvoj0yIcffsiuXbtYv349AIMHD2bw4MHehpKIaXhHRHrk6NGjpKen85WvfIWJEyfy0EMPcfr0aa9jSYRU+iISkYrqAJ8pf427n3idP+zZw4Q776G6uporrriC8vJyr+NJhFT6InJJ7ZPmBZqaSRk6gpQrR/Ds4UFUVAe49957efPNN72OKBHSmL6IXFLopHkpV17NwGEjOPXOX1i1fTDFZ3cxYcIEjxNKpGJe+mY2A1gDpABPOef0d6BIkgmfNG/4XV/l3a2P09jaQvr/vJVnn33Wo2TSUzEtfTNLAX4CFAP1wB/MbItz7lAs1ysi0ZWZlkogpPgHj7yBUQtWk5WWSsXSOz1MJj0V6zH9KcAR59zbzrnzwEZgdozXKSJRpknz+o9Yl34WUBfyuD64rIOZLTGzKjOramxsjHEcEemNOROzWFGaT1ZaKgZkpaWyojRf8yklIc+/yHXOrQPWARQWFjqP44hINzRpXv8Q6z39AHBtyOPs4DIREfFArEv/D8BYM7vezAYD9wNbYrxOERHpRkyHd5xzLWb2dWA7bYdsPuOcOxjLdYqISPdiPqbvnNsGbIv1ekRE5NI0DYOIiI+o9MWX1qxZQ15eHrm5uaxevdrrOCJxo9IX36mpqeHJJ59k9+7d7Nu3j61bt3LkyBGvY4nEhUpffOett97itttuY8iQIQwcOJA77riDTZs2eR1LJC5U+uI7eXl5/OY3v+G9997jzJkzbNu2jbq6ukv/oEg/4PkZuSLxUlEdYNX2Wo43NZOSP4spn51KVnoaBQUFpKSkXPoNRPoBlb74QvtFQNrnhG8ZO5XUCXfx7dJ8dr/4E7Kzsz1OKBIfKn3xhdCLgAC0nm6i+Yo0/uX/7uLjik288cYbHqYTiR+VvvhC+EVAGiv+lYvNH3F8QAovP/8UaWlpHiUTiS+VvvhC+EVArpm/EmibInjaNF0ERPxDR++IL+giICJttKcvvtA+D3z70TuZaamUleRofnjxHZW++IYuAiKi4R0REV9R6YuI+IhKX0TER1T6IiI+otIXEfERlb6IiI+o9EVEfESlLyLiIyp9EREfUemLiPiISl9ExEdU+iIiPqLSFxHxEZW+iIiPqPRFRHxEpS8i4iMqfRERH+lXpX/27FmmTJnCrbfeSm5uLo8++qjXkUREEkrMSt/M/tnMAma2N3i7O1branfZZZfx2muvsW/fPvbu3UtlZSVvvPFGrFcrIpI0Yn2N3Cecc4/HeB0dzIwrr7wSgAsXLnDhwgXMLF6rFxFJeP1qeAegtbWVgoICMjIyKC4u5rbbbvM6kohIwoj1nv7XzewfgCrgfznnPoj2CiqqA6zaXsvxpmYy01IpK8lh7969NDU1MXfuXGpqasjLy4v2akVEklKf9vTN7FdmVtPFbTbwH8CNQAHQAPywm/dYYmZVZlbV2NjYo/VXVAdYtukAgaZmHBBoambZpgNUVAdIS0tj6tSpVFZW9uVXFBHpV/pU+s65u5xzeV3cNjvnTjjnWp1zF4EngSndvMc651yhc64wPT29R+tftb2W5gutHY9bz3zI6Y8+bFve3Myrr77KuHHj+vIrioj0KzEb3jGzUc65huDDuUBNtNdxvKm50+PWj9/n3Zee4B13kb997grmzZvHzJkzo71aEZGkFcsx/ZVmVgA44Bjwj9FeQWZaKoGQ4h+ccT2ZX/kRWWmp/HbpndFenYhI0ovZ0TvOub93zuU7525xzs0K2euPmrKSHFIHpXRaljoohbKSnGivSkSkX4j10TsxNWdiFsAnjt5pXy4ikmzGjBnD0KFDSUlJYeDAgVRVVUX1/ZO69KGt+FXyItKf7Ny5kxEjRsTkvfvdyVkiItI9lb6ISAIxM6ZPn87kyZNZt25d1N8/6Yd3RESSWfisAv977YssLink5MmTFBcXM27cOIqKiqK2Pu3pi4h4pKtZBR7/r3epqA6QkZHB3Llz2b17d1TXqdIXEfFI+KwCF8+f5fTHH7Fqey2nT5/mlVdeifrcYRreERHxyCdmFTjTROOmf+EdYMrPhvDlL3+ZGTNmRHWdKn0REY+EzyowKO0aMhf9OKazCmh4R0TEI17MKqDSv4S6ujqmTp3KhAkTyM3NZc2aNV5HEpF+Ys7ELFaU5pOVlooBWWmprCjNj+kJp+aci9mb91RhYaGL9inHfdXQ0EBDQwOTJk3io48+YvLkyVRUVDBhwgSvo4mIAGBme5xzhZG8Vnv6lzBq1CgmTZoEwNChQxk/fjyBQMDjVCIivaPS74Fjx45RXV2t6+6KSNLS0Ttd6Oq6u3eNvYp77rmH1atXM2zYMK8jioj0iko/TPsZcu0nTASamln6YjWpOx/n7+fPp7S01OOEIiK9p+GdMOFnyDnnqN/yb5wY8Dd8+9vf9jCZiEjfqfTDhJ8hdy5wiNMHd/Lu4TcpKCigoKCAbdu2eZRORKRvNLwTJvwMucuzcxn9T1t13V0R6Re0px9G190Vkf5Me/phdN1dEenPVPpd0HV3RaS/0vCOiIiPqPRFRHxEpS8i4iMqfRERH1Hpi4j4iEpfRMRHVPoiIj6i0hcR8RGVvoiIj6j0RUR8pE+lb2b3mdlBM7toZoVhzy0zsyNmVmtmJX2LKSIi0dDXPf0aoBTYFbrQzCYA9wO5wAzg380s5ZM/LiLSfy1atIiMjAzy8vK8jtKhT6XvnHvLOVfbxVOzgY3OuXPOuaPAEWBKX9YlIpJsFi5cSGVlpdcxOonVmH4WUBfyuD647BPMbImZVZlZVWNjY4ziiIjEX1FREcOHD/c6RieXnFrZzH4FXNPFU4845zb3NYBzbh2wDqCwsND19f1ERKR7lyx959xdvXjfAHBtyOPs4DIRkX6tojrQ6SJMC/KHeB2pk1gN72wB7jezy8zsemAssDtG6xIRSQgV1QGWbTpAoKkZBwSamvlBZS2nzrZ4Ha1DXw/ZnGtm9cD/AF4ys+0AzrmDwAvAIaAS+JpzrrWvYUVEEtmq7bU0X+hcdedaWnn343MeJfqkvh6980vnXLZz7jLn3EjnXEnIc8udczc653Kccy/3PaqISGI73tTc6XHjlpW887Pv0NxYR3Z2Nk8//bRHyf5K18gVEYmSzLRUAiHFnz7rYQCy0lL57dI7vYrViaZhEBGJkrKSHFIHdT4PNXVQCmUlOR4l+iTt6YuIRMmciW2nI4UevVNWktOxPBGo9EVEomjOxKyEKvlwGt4REfERlb6IiI+o9EVEfESlLyLiIyp9EREfUemLiG9VVlaSk5PDTTfdRHl5uddx4kKlLyK+1Nrayte+9jVefvllDh06xPPPP8+hQ4e8jhVzKn0R8aXdu3dz0003ccMNNzB48GDuv/9+Nm/u8yVCEp5KX0R8KRAIcO21f73sR3Z2NoFA/7/sh87IFRHfCL3AyeC6A1x35rTXkeJOe/oi4gvhFzg5NWAov9//Jyqq2/bu6+vrycpK3OkTokWlLyK+EH6Bk8GjbubcewG+//zrnD9/no0bNzJr1iwPE8aHSl9EfCH8Aic2IIXhxV9l/1MPM378eObNm0dubq5H6eJHY/oi4gvhFzgBSL3xb5kyuShhLnASD9rTFxFfSIYLnMSD9vRFxBeS4QIn8aDSFxHfSPQLnMSDhndERHxEpS8i4iMqfRERH1Hpi4j4iEpfRMRHzDnndYYOZtYI/CWCl44A3o1xnFhQ7vhS7vhS7vhrzz7aOZceyQ8kVOlHysyqnHOFXufoKeWOL+WOL+WOv95k1/COiIiPqPRFRHwkWUt/ndcBekm540u540u546/H2ZNyTF9ERHonWff0RUSkF1T6IiI+klSlb2arzOyPZrbfzH5pZmkhzy0zsyNmVmtmJV7mDGdm95nZQTO7aGaFIcvHmFmzme0N3tZ6mTNcd7mDzyXs9g5lZv9sZoGQbXy315k+jZnNCG7TI2a21Os8kTKzY2Z2ILiNq7zO0x0ze8bMTppZTciy4Wb2qpkdDv57tZcZu9JN7l59tpOq9IFXgTzn3C3An4BlAGY2AbgfyAVmAP9uZindvkv81QClwK4unvuzc64gePtqnHNdSpe5k2B7h3siZBtv8zpMd4Lb8CfA54EJwAPBbZ0spga3cSIf876ets9sqKXADufcWGBH8HGiWc8nc0MvPttJVfrOuVeccy3Bh28A2cH7s4GNzrlzzrmjwBFgihcZu+Kce8s5V+t1jp76lNwJvb2T2BTgiHPubefceWAjbdtaosQ5twt4P2zxbGBD8P4GYE5cQ0Wgm9y9klSlH2YR8HLwfhZQF/JcfXBZMrjezKrN7Ndm9ndeh4lQsm3vrweHBJ9JxD/dQyTbdg3lgFfMbI+ZLfE6TA+NdM41BO+/A4z0MkwP9fiznXBXzjKzXwHXdPHUI865zcHXPAK0AP8nntk+TSS5u9AAXOece8/MJgMVZpbrnDsVs6Bhepk7oXza7wD8B/B92krp+8APadthkOj6rHMuYGYZwKtm9sfg3mlScc45M0uW49h79dlOuNJ3zt31ac+b2UJgJjDN/fUkgwBwbcjLsoPL4uZSubv5mXPAueD9PWb2Z+BmIG5fhPUmNwmwvUNF+juY2ZPA1hjH6YuE2q494ZwLBP89aWa/pG2oKllK/4SZjXLONZjZKOCk14Ei4Zw70X6/J5/tpBreMbMZwMPALOfcmZCntgD3m9llZnY9MBbY7UXGnjCz9PYvQM3sBtpyv+1tqogkzfYO/k/cbi5tX04nqj8AY83sejMbTNuX5Vs8znRJZnaFmQ1tvw9MJ7G3c7gtwILg/QVAsvyF27vPtnMuaW60fWFYB+wN3taGPPcI8GegFvi811nDcs+lbXz2HHAC2B5cfg9wMPi7vAl80euskeRO9O0d9jv8DDgA7Kftf+5RXme6RN67aTsy7c+0DbF5nimCzDcA+4K3g4mcG3ietmHVC8HP9mLgb2g7aucw8CtguNc5I8zdq8+2pmEQEfGRpBreERGRvlHpi4j4iEpfRMRHVPoiIj6i0hcR8RGVvoiIj6j0RUR85P8DiM8hY0u58/oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPzxvS787Vs5",
        "outputId": "c7b8cc4b-bc3b-4782-d1cc-4f98a754c094"
      },
      "source": [
        "emb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqJfMyq-nUKG"
      },
      "source": [
        "train_emb = np.zeros((X_train.shape[0], emb.shape[1]))\n",
        "for seq in X_train:\n",
        "  translated_seq = [node_to_ind[e] for e in seq]\n",
        "  for ind in translated_seq:\n",
        "    #print(ind)\n",
        "    train_emb[i] += emb[ind]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-Aq40rY7k2c",
        "outputId": "29584f6c-0a75-4991-884d-cd3e6d1ce35f"
      },
      "source": [
        "tsne_emb = TSNE(n_components = 2, perplexity = 5, learning_rate = 10).fit_transform(train_emb)\n",
        "\n",
        "plt.scatter(tsne_emb[:,0], tsne_emb[:,1])\n",
        "for i in range(len(tsne_emb)):\n",
        "    plt.annotate(str(i), xy=(tsne_emb[i,0], tsne_emb[i,1]))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLTPTNkD7-oW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}